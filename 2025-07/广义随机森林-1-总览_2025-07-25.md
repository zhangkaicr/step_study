# 广义随机森林



![教程首页](https://files.mdnice.com/user/36552/a5d6a533-7a5d-4fee-a3c9-144dfcc6c0ea.png)

一个基于森林的统计估计和推断包。G**RF 提供了用于估计异质处理效应的非参数方法（可选使用右删失结果、多个处理组或结果，或工具变量），以及最小二乘回归、分位数回归和生存回归，所有这些都支持缺失协变量。**

此外，GRF 支持“诚实”估计（其中数据的一个子集用于选择分割，另一个子集用于填充树的叶子），以及最小二乘回归和处理效应估计的置信区间。

一些有助于入门的链接：
- R 包文档包含使用示例和方法参考。
- GRF 参考文献给出了 GRF 算法的详细描述，并包括故障排除建议。
- 有关使用的社区问答，请参见标记为“问题”的 Github 问题。
该存储库最初是 ranger 存储库的分支——我们非常感谢 ranger 的作者们提供了这个有用且免费的包。

## 安装
最新版本的包可以通过 CRAN 安装：
```r
install.packages("grf")
```

`conda` 用户可以从 conda-forge 通道安装：
```bash
conda install -c conda-forge r-grf
```

当前开发版本可以使用 devtools 从源代码安装。
```r
devtools::install_github("grf-labs/grf", subdir = "r-package/grf")
```

请注意，要从源代码安装，需要实现 C++11 或更高版本的编译器。如果在 Windows 上安装，还需要 RTools 工具链。

## 使用示例
以下脚本演示了如何使用 GRF 进行异质处理效应估计。有关如何使用其他类型森林的示例，请查阅相关方法的 R 文档。

```r
library(grf)

# 生成数据
# 设置样本数量为2000，即数据集中包含2000个观测值
n <- 2000

# 设置特征数量为10，即每个观测值由10个特征组成
p <- 10

# 生成一个n行p列的矩阵X，矩阵中的元素服从标准正态分布（均值为0，标准差为1）
# 这里使用rnorm函数生成n * p个随机数，然后将其排列成n行p列的矩阵
X <- matrix(rnorm(n * p), n, p)

# 生成一个用于测试的矩阵X.test，该矩阵有101行p列，初始所有元素都设为0
X.test <- matrix(0, 101, p)
# 将X.test矩阵的第一列元素设置为从-2到2的等间距数值，共101个
X.test[, 1] <- seq(-2, 2, length.out = 101)

# 生成一个长度为n的二元变量W，它表示每个观测值的处理状态（例如是否接受某种处理）
# rbinom函数用于生成二项分布的随机数，试验次数为1，成功概率为0.4 + 0.2 * (X[, 1] > 0)
# 即当X矩阵第一列的元素大于0时，成功概率为0.6；否则为0.4
W <- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] > 0))

# 生成响应变量Y，它是根据以下规则计算得到的：
# 1. pmax(X[, 1], 0) * W：取X矩阵第一列元素与0的较大值，再乘以处理状态W
# 2. X[, 2]：加上X矩阵第二列的元素
# 3. pmin(X[, 3], 0)：加上X矩阵第三列元素与0的较小值
# 4. rnorm(n)：最后加上n个服从标准正态分布的随机噪声
Y <- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)

```

![](https://files.mdnice.com/user/36552/5a0e7cb1-e11e-411e-9d54-495332bd73da.png)


![](https://files.mdnice.com/user/36552/7d819c4a-a55a-48d1-8b24-96c269dc6529.png)

![](https://files.mdnice.com/user/36552/efcee604-3ed1-4c61-9296-754e8cc6dadf.png)


![](https://files.mdnice.com/user/36552/11729ad5-afef-49b4-80d9-8d1d8c13c7b0.png)

```
# 使用 causal_forest 函数训练因果森林模型
# X：特征矩阵，包含所有观测样本的特征数据
# Y：响应变量，即模型的输出结果
# W：处理状态变量，指示每个样本是否接受处理
# num.trees = 2000：设置因果森林中树的数量为2000棵
# min.node.size = 5：设置每个节点的最小样本数量为5
# honesty = TRUE：开启 honesty 方法
# honesty.fraction = 0.9：设置 honesty 方法中用于训练模型的样本比例为0.9
tau.forest <- causal_forest(
  X,          # 特征矩阵，包含所有观测样本的特征数据
  Y,          # 响应变量，即模型的输出结果
  W,          # 处理状态变量，指示每个样本是否接受处理
  num.trees = 2000,  # 设置因果森林中树的数量为2000棵
  min.node.size = 5, # 设置每个节点的最小样本数量为5
  honesty = TRUE,    # 开启 honesty 方法
  honesty.fraction = 0.9  # 设置 honesty 方法中用于训练模型的样本比例为0.9
)
```

![](https://files.mdnice.com/user/36552/3c724265-9986-4df2-8ef6-551915897822.png)

```
# 使用袋外预测估计训练数据的处理效应
tau.hat.oob <- predict(tau.forest)
hist(tau.hat.oob$predictions)
dim(tau.hat.oob)
head(tau.hat.oob)
```

![](https://files.mdnice.com/user/36552/93765f55-e034-4dcd-801a-b557ce889bb0.png)

![](https://files.mdnice.com/user/36552/6b5f890d-25fa-4bcd-885a-14a8efc7a27a.png)

```


# 估计测试样本的处理效应。
tau.hat <- predict(tau.forest, X.test)
plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2), xlab = "x", ylab = "tau", type = "l")
lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 2)

```


![](https://files.mdnice.com/user/36552/38cd54a2-b676-4622-a33d-5dddeda51533.png)



![](https://files.mdnice.com/user/36552/1b6e41ea-2b59-488a-b523-df31e995297a.png)



![](https://files.mdnice.com/user/36552/c50694f1-9255-4619-833e-dd5bc1d417b1.png)


```
# 估计全样本的条件平均处理效应（CATE）
average_treatment_effect(tau.forest, target.sample = "all")

# 估计处理样本的条件平均处理效应（CATT）
average_treatment_effect(tau.forest, target.sample = "treated")
```

![目前人群的参数设定](https://files.mdnice.com/user/36552/b0cf44be-f2d7-4dba-873a-abe5aec646b6.png)


![](https://files.mdnice.com/user/36552/255e228d-beab-419b-beeb-ddbc1d47f21e.png)

```
# 为异质处理效应添加置信区间；现在建议增加树的数量。
# 重新训练因果森林模型，将树的数量增加到4000棵，以提高模型的稳定性和预测精度
tau.forest <- causal_forest(X, Y, W, num.trees = 4000)
# 对测试数据进行预测，并设置 estimate.variance = TRUE 以估计预测值的方差
# 这样可以得到每个预测值对应的方差估计，用于后续计算置信区间
tau.hat <- predict(tau.forest, X.test, estimate.variance = TRUE)
# 计算预测值的标准差，通过对方差估计值取平方根得到
# 标准差是计算置信区间的关键参数
sigma.hat <- sqrt(tau.hat$variance.estimates)
# 绘制测试数据第一列特征与预测处理效应的关系图
# ylim 参数设置y轴的范围，包含预测值的上下置信区间边界、0和2中的最小值与最大值
# xlab 和 ylab 分别设置x轴和y轴的标签
# type = "l" 表示绘制折线图
plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2), xlab = "x", ylab = "tau", type = "l")
# 在已有的图上添加预测值的上置信区间折线
# 置信区间为预测值加上1.96倍的标准差（对应95%置信水平）
# col = 1 表示折线颜色为黑色，lty = 2 表示折线类型为虚线
lines(X.test[, 1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
# 在已有的图上添加预测值的下置信区间折线
# 置信区间为预测值减去1.96倍的标准差（对应95%置信水平）
# col = 1 表示折线颜色为黑色，lty = 2 表示折线类型为虚线
lines(X.test[, 1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
# 在已有的图上添加一条新的折线
# 该折线表示X.test第一列元素与0的较大值
# col = 2 表示折线颜色为红色，lty = 1 表示折线类型为实线
lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 1)

```

![CATE的可信区间](https://files.mdnice.com/user/36552/03694698-61e1-458b-ad0f-0dda3c246d5a.png)

![预测数据包括的方差](https://files.mdnice.com/user/36552/a4346d50-a5a6-431e-8f72-c0c3d742b99c.png)


```
# 利用ggplot2语法绘制图形
# 加载所需库
library(ggplot2)
library(dplyr)

# 准备绘图数据
data_plot <- tibble(
  x = X.test[, 1],
  predictions = tau.hat$predictions,
  upper_ci = tau.hat$predictions + 1.96 * sigma.hat,
  lower_ci = tau.hat$predictions - 1.96 * sigma.hat,
  theoretical = pmax(0, X.test[, 1])
)

# 使用ggplot2绘制图形
ggplot(data_plot, aes(x = x)) +
  # 添加预测值曲线
  geom_line(aes(y = predictions), color = "black") +
  # 添加置信区间曲线
  geom_line(aes(y = upper_ci), color = "black", linetype = "dashed") +
  geom_line(aes(y = lower_ci), color = "black", linetype = "dashed") +
  # 添加理论值曲线
  geom_line(aes(y = theoretical), color = "red") +
  # 设置坐标轴标签
  labs(x = "x", y = "tau") +
  # 设置y轴范围与原plot保持一致
  ylim(range(data_plot$upper_ci, data_plot$lower_ci, 0, 2)) +
  # 使用经典主题，接近基础plot的外观
  theme_classic()

```

![](https://files.mdnice.com/user/36552/038cc880-3677-4766-9758-d511be94fb86.png)


```


# 在某些示例中，分别预拟合 Y 和 W 的模型可能会有帮助
#（例如，如果不同的模型使用不同的协变量）
# 在某些应用中，人们甚至可能希望使用完全不同的方法（例如， boosting）来获得 Y.hat 和 W.hat

# 生成新数据
# 设置样本数量为4000，即数据集中包含4000个观测值
n <- 4000
# 设置特征数量为20，即每个观测值由20个特征组成
p <- 20
# 生成一个n行p列的矩阵X，矩阵中的元素服从标准正态分布（均值为0，标准差为1）
X <- matrix(rnorm(n * p), n, p)
# 计算处理效应TAU，使用X矩阵第三列元素通过sigmoid函数转换得到
TAU <- 1 / (1 + exp(-X[, 3]))
# 生成一个长度为n的二元变量W，它表示每个观测值的处理状态
# 使用sigmoid函数基于X矩阵第一列和第二列元素之和计算成功概率，生成二项分布随机数
W <- rbinom(n, 1, 1 / (1 + exp(-X[, 1] - X[, 2])))
# 生成响应变量Y
# 1. pmax(X[, 2] + X[, 3], 0)：取X矩阵第二列与第三列元素之和与0的较大值
# 2. rowMeans(X[, 4:6]) / 2：取X矩阵第4到6列元素的行均值并除以2
# 3. W * TAU：处理状态W乘以处理效应TAU
# 4. rnorm(n)：加上n个服从标准正态分布的随机噪声
Y <- pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)
```

![](https://files.mdnice.com/user/36552/6a9c386c-e10b-46a6-9e03-5c76363cc448.png)

![](https://files.mdnice.com/user/36552/e8ce3127-7a50-4942-bd7a-1dd86d734269.png)

```

# 使用 regression_forest 函数训练一个回归森林模型，用于预测处理状态变量 W
# X 为特征矩阵，包含所有观测样本的特征数据
# W 为响应变量，即每个样本的处理状态
# tune.parameters = "all" 表示自动调整所有可调整的参数以优化模型性能
forest.W <- regression_forest(X, W, tune.parameters = "all")
forest.W
# 使用训练好的回归森林模型 forest.W 对训练数据进行预测
# $predictions 用于提取预测结果，将预测得到的处理状态值存储在 W.hat 中
W.hat <- predict(forest.W)$predictions
head(W.hat)

# 使用 regression_forest 函数训练一个回归森林模型，用于预测响应变量 Y
# X 为特征矩阵，包含所有观测样本的特征数据
# Y 为响应变量，即模型的输出结果
# tune.parameters = "all" 表示自动调整所有可调整的参数以优化模型性能
forest.Y <- regression_forest(X, Y, tune.parameters = "all")

# 使用训练好的回归森林模型 forest.Y 对训练数据进行预测
# $predictions 用于提取预测结果，将预测得到的响应变量值存储在 Y.hat 中
Y.hat <- predict(forest.Y)$predictions
head(Y.hat)

# 使用 variable_importance 函数计算回归森林模型 forest.Y 中各特征变量的重要性
# 将计算得到的特征重要性结果存储在 forest.Y.varimp 中
forest.Y.varimp <- variable_importance(forest.Y)
head(forest.Y.varimp)
```

![治疗分配的条件期望预测结果-PS评分结果](https://files.mdnice.com/user/36552/e43a2dab-34ec-424b-82cb-a522c373f0e4.png)


![regression_forest的用法](https://files.mdnice.com/user/36552/1adb60c5-0ea3-41ad-8813-527e52d405f3.png)


![所有森林情况](https://files.mdnice.com/user/36552/2564a16e-17bb-4ac7-b6e8-6525e4694d55.png)


![](https://files.mdnice.com/user/36552/c077080c-07e4-4484-89f2-65cc5fbd37ba.png)

![](https://files.mdnice.com/user/36552/084fbe5a-af03-4b9d-a99e-823db62268c0.png)





```

# 注意：当森林在很少的变量上训练时可能会遇到困难
#（例如，ncol(X) = 1、2 或 3）。我们建议不要过于激进地进行选择

# 筛选出特征重要性大于平均重要性 0.2 倍的特征索引
selected.vars <- which(forest.Y.varimp / mean(forest.Y.varimp) > 0.2)

# 使用筛选后的特征训练因果森林模型，同时传入处理状态和响应变量的预测值，并自动调整所有参数
# X[, selected.vars]：使用筛选出的特征重要性大于平均重要性 0.2 倍的特征构建的特征矩阵
# Y：响应变量，即模型的输出结果
# W：处理状态变量，指示每个样本是否接受处理
# W.hat：处理状态变量 W 的预测值，由回归森林模型 forest.W 预测得到
# Y.hat：响应变量 Y 的预测值，由回归森林模型 forest.Y 预测得到
# tune.parameters = "all"：自动调整所有可调整的参数以优化模型性能
tau.forest <- causal_forest(X[, selected.vars], Y, W,
                            W.hat = W.hat, Y.hat = Y.hat,
                            tune.parameters = "all")
```

## AI知识补充-为什么模型中要放入W.hat和Y.hat 
          
在因果森林模型 <mcsymbol name="causal_forest" filename="grf-1-总览.R" path="c:/Users/85330/Desktop/study/grf-1-总览.R" startline="210" type="function"></mcsymbol> 中传入 `W.hat` 和 `Y.hat` 的核心目的是**提高因果效应估计的稳健性和效率**，具体原因如下：

### 1. **W.hat（处理倾向得分的预测值）**
- **作用**：控制处理分配的非随机性，减少混杂偏倚
- **原理**：
  - `W.hat` 是通过回归森林 `forest.W` 预估计的处理概率（0-1之间）
  - 在观察性数据中，处理分配（W）可能与协变量相关，直接估计处理效应会受混杂影响
  - 传入 `W.hat` 相当于引入倾向得分（propensity score）调整，使模型更专注于条件平均处理效应（CATE）的估计
- **类比**：类似倾向性得分匹配/加权的思想，但通过机器学习方法实现更灵活的调整

### 2. **Y.hat（结果变量的预测值）**
- **作用**：控制基线协变量对结果的独立影响，分离处理效应
- **原理**：
  - `Y.hat` 是通过回归森林 `forest.Y` 预估计的潜在结果（不考虑处理效应时的Y值）
  - 因果森林的核心是估计 `Y - Y.hat` 中由处理W带来的增量部分
  - 预估计 `Y.hat` 可以过滤掉协变量对结果的线性/非线性影响，使模型更准确识别处理效应
- **优势**：避免因果森林同时承担预测基线结果和处理效应的双重任务，提高估计精度

### 3. **理论依据：Double Machine Learning框架**
这种两步估计方法（先预估计干扰参数 `W.hat`/`Y.hat`，再估计因果效应）符合双重机器学习理论，能够：
- 降低模型对高维协变量的过拟合风险
- 保证在弱假设下的统计推断有效性
- 提升异质处理效应估计的稳定性

### 4. **计算效率优化**
- 预估计 `W.hat` 和 `Y.hat` 可以复用已有模型结果，避免因果森林内部重复计算
- 允许使用更适合预测W/Y的模型（如正则化回归、梯度提升树），而非局限于随机森林

### 总结
通过传入这两个预估计值，模型实现了**干扰参数与核心参数的分离估计**，既控制了混杂因素，又提高了处理效应估计的统计效率，特别适合在高维协变量或复杂非线性关系的数据中应用。
        

```
# 通过绘制 TOC 并计算 AUTOC 的 95% 置信区间，查看因果森林是否成功捕捉到异质性。
# 从 1 到 n 的整数中随机抽样，抽取样本数量为 n/2，将抽样结果存储在 train 中，用于划分训练集索引
train <- sample(1:n, n / 2)

# 使用训练集索引 train 从特征矩阵 X、响应变量 Y 和处理状态变量 W 中提取对应的数据，训练一个因果森林模型
train.forest <- causal_forest(X[train, ], Y[train], W[train])
# 使用除训练集索引之外的数据（即测试集）从特征矩阵 X、响应变量 Y 和处理状态变量 W 中提取对应的数据，训练另一个因果森林模型
eval.forest <- causal_forest(X[-train, ], Y[-train], W[-train])
# 计算排序平均处理效应，使用评估森林模型 eval.forest 和训练森林模型在测试集上的预测结果
rate <- rank_average_treatment_effect(eval.forest,
                                      predict(train.forest, X[-train, ])$predictions)
# 绘制排序平均处理效应的结果
plot(rate)

# 输出 AUTOC（排序平均处理效应）的估计值及其 95% 置信区间的误差范围，结果保留两位小数
paste("AUTOC:", round(rate$estimate, 2), "+/", round(1.96 * rate$std.err, 2))
```


![](https://files.mdnice.com/user/36552/668f780d-8150-4785-9f07-d26609d67ad6.png)



![](https://files.mdnice.com/user/36552/d3726209-13d7-45cf-9ae6-0ff2d8f755e6.png)



### 一、TOC曲线的统计学原理
#### 1. **核心用途**
- 评估**靶向治疗策略**的效果：根据个体处理效应（CATE）排序后，选择不同比例的高响应人群接受处理，观察平均处理效应的变化。
- 量化**处理效应异质性**：曲线越陡峭，表明存在显著的人群亚组差异（部分人群从处理中获益更多）。

#### 2. **坐标轴定义**
- **横轴（Treated fraction, q）**：按预测CATE从高到低排序后，接受处理的人群比例（0到1）。
- **纵轴**：对应比例下的**平均处理效应估计值**。
- **虚线**：95%置信区间，反映估计值的统计不确定性。

#### 3. **与经典方法的区别**
传统方法（如ATE）仅估计整体平均效应，而TOC曲线通过以下方式实现个体化评估：
```plaintext
1. 用因果森林（causal_forest）估计每个样本的CATE
2. 按CATE排序后分位数分组（q=0.1, 0.2,...1.0）
3. 计算每组实际观测到的平均处理效应
4. 绘制不同q值与效应的关系曲线
```
<mcurl name="GRF官方文档-RATE方法" url="https://grf-labs.github.io/grf/articles/rate.html"></mcurl>

### 二、GRF如何支持TOC曲线绘制
#### 1. **关键技术：异质性分裂准则**
GRF通过**最大化子节点处理效应差异**构建树结构，具体：
- 传统随机森林按结果变量（Y）的方差分裂
- 因果森林按**处理效应（Y(1)-Y(0)）的异质性**分裂，公式近似为：
  ```plaintext
  分裂质量 ∝ |τ_left - τ_right|²
  ```
  其中τ为节点内平均处理效应<mcurl name="GRF算法原理" url="https://grf-labs.github.io/grf/articles/grf_guide.html"></mcurl>

#### 2. **核心函数链**
```r
# 1. 训练因果森林
tau.forest <- causal_forest(X, Y, W, tune.parameters = "all")

# 2. 估计CATE并计算RATE
rate_result <- rank_average_treatment_effect(tau.forest)

# 3. 绘制TOC曲线（需结合ggplot2）
plot(rate_result)
```
<mcurl name="RATE函数文档" url="https://grf-labs.github.io/grf/reference/rank_average_treatment_effect.html"></mcurl>

### 三、您提供的TOC曲线解读
#### 1. **曲线特征**
- **实线（估计效应）**：在q=0.2左右达到峰值（~0.3），表明选择CATE最高的20%人群处理时，平均效应最大。
- **下降趋势**：随着q增大（纳入更多低响应人群），平均效应逐渐降低至0，说明处理仅对特定亚组有效。
- **置信区间（虚线）**：在q<0.8时不包含0，表明效应估计具有统计显著性。

#### 2. **实际应用价值**
- **临床决策**：可确定最优治疗人群比例（如仅对CATE前20%患者给予干预）。
- **政策评估**：评估靶向干预措施的成本效益（如资源有限时优先覆盖高响应人群）。

### 四、与其他可视化工具的区别
| 方法         | 优势                                  | GRF-TOC曲线特点                  |
|--------------|---------------------------------------|----------------------------------|
| ROC曲线      | 评估分类器性能                        | 专注于处理效应的亚组差异         |
| 森林特征重要性 | 识别关键变量                          | 直接量化处理效应的异质性程度     |
| Q-Q图        | 检验分布假设                          | 提供实际可操作的靶向治疗比例指导 |

### 总结
GRF的TOC曲线通过**个体化处理效应估计→排序→分组验证**的流程，为精准干预提供了量化工具。其核心创新在于将因果推断与机器学习结合，既保留了森林模型的灵活性，又通过统计理论保证了推断有效性。实际应用中，需结合置信区间和临床意义综合解读结果。
        

## 开发版功能

除了提供现成的分位数回归和因果效应估计森林外，GRF 还提供了一个框架，用于创建适合新统计任务的森林。如果您想使用 GRF 进行开发，请查阅算法参考和开发指南。



## 参考文献



- Susan Athey and Stefan Wager. Estimating Treatment Effects with Causal Forests: An Application. Observational Studies, 5, 2019. [paper, arxiv]

- Susan Athey, Julie Tibshirani and Stefan Wager. Generalized Random Forests. Annals of Statistics, 47(2), 2019. [paper, arxiv]

- Yifan Cui, Michael R. Kosorok, Erik Sverdrup, Stefan Wager, and Ruoqing Zhu. Estimating Heterogeneous Treatment Effects with Right-Censored Data via Causal Survival Forests. Journal of the Royal Statistical Society: Series B, 85(2), 2023. [paper, arxiv]

- Rina Friedberg, Julie Tibshirani, Susan Athey, and Stefan Wager. Local Linear Forests. Journal of Computational and Graphical Statistics, 30(2), 2020. [paper, arxiv]

- Imke Mayer, Erik Sverdrup, Tobias Gauss, Jean-Denis Moyer, Stefan Wager and Julie Josse. Doubly Robust Treatment Effect Estimation with Missing Attributes. Annals of Applied Statistics, 14(3), 2020. [paper, arxiv]

- Erik Sverdrup, Maria Petukhova, and Stefan Wager. Estimating Treatment Effect Heterogeneity in Psychiatry: A Review and Tutorial with Causal Forests. International Journal of Methods in Psychiatric Research, 34(2), 2025. [paper, arxiv]

- Stefan Wager. Causal Inference: A Statistical Learning Approach. 2024. [pdf]

- Stefan Wager and Susan Athey. Estimation and Inference of Heterogeneous Treatment Effects using Random Forests. Journal of the American Statistical Association, 113(523), 2018. [paper, arxiv]

- Steve Yadlowsky, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects. Journal of the American Statistical Association, 120(549), 2025. [paper, arxiv]


------------------



### 如果您对真实世界研究/因果推断/生信分析/影像组学/人工智能算法感兴趣可以**通过下方的微信加我的交流群**


![助教微信-程老师](https://files.mdnice.com/user/36552/42b3702f-74f0-4ad8-8ff1-c36656a37150.jpg)

![助教微信-金老师](https://files.mdnice.com/user/36552/4bf8b757-2907-4179-b5df-14419eb92e90.jpg)



### 欢迎关注我的视频号-**定期直播免费文献分享会**


![扫一扫，添加我的视频号](https://files.mdnice.com/user/36552/bb758dda-5031-48df-a061-51f549c5b61b.jpg)


## 我的小红书


![](https://files.mdnice.com/user/36552/781dab80-0990-40e6-94c2-38550b39b066.jpg)

## 我的抖音平台



![](https://files.mdnice.com/user/36552/d04de1cb-07bb-4c58-97cf-1cd78a9088c0.png)



### 欢迎关注我的B站账号-**公开课及文献分享视频会更新至此**



![我的B站](https://files.mdnice.com/user/36552/167c6dc1-6d6e-40b3-b6c8-3379489d5c2e.jpg)









