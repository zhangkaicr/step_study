{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146c39a-f484-4a47-b9e6-13050d200797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import precision_score, confusion_matrix, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn import metrics\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import learning_curve\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "#显示所有列，把行显示设置成最大\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ed981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b77fd5-2a6c-4d11-84e9-24e72b4dd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing(data):\n",
    "    missing_num = data.isna().sum(axis=0).sort_values(ascending=False)\n",
    "    missing_prop = missing_num/float(len(data)) \n",
    "    drop_index = missing_prop[missing_prop>0.3].index.tolist() \n",
    "    return drop_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8242472-7a1b-40ab-9853-9f694f8574ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "variance_1 = VarianceThreshold(threshold = (0.90*(1-0.90))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e647f4c-a515-4294-ad00-e468e1e9ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature3['中性粒细胞/淋巴细胞计数(NLR)'] = feature3['中性粒细胞/淋巴细胞计数(NLR)'].replace(np.inf,np.nan)\n",
    "feature3['血小板/淋巴细胞比值(PLR)'] = feature3['血小板/淋巴细胞比值(PLR)'].replace(np.inf,np.nan)\n",
    "feature3['淋巴细胞/单核细胞比值(LMR)'] = feature3['淋巴细胞/单核细胞比值(LMR)'].replace(np.inf,np.nan)\n",
    "feature3['血小板/白蛋白比值(PAR)'] = feature3['血小板/白蛋白比值(PAR)'].replace(np.inf,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_neighbors':  np.arange(40,500,20),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    }\n",
    "    \n",
    "\n",
    "imputer_knn = KNNImputer()\n",
    "\n",
    "random_search = RandomizedSearchCV(imputer_knn, param_distributions=param_dist, cv=5, scoring='roc_auc',n_iter=10) \n",
    "\n",
    "random_search.fit(feature3[continuous_features3] , y['是否在28天内死亡'])\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_best_knn = KNNImputer(**best_params)\n",
    "features_imputed_knn = imputer_best_knn.fit_transform(feature3[continuous_features3])\n",
    "\n",
    "df_imputed_knn = pd.DataFrame(features_imputed_knn, columns=feature3[continuous_features3].columns)\n",
    "df_imputed_knn = pd.concat([df_imputed_knn, feature3[categorical_features3]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42885f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_imputed_knn, y['是否在28天内死亡'], test_size=0.3)\n",
    "\n",
    "rf = RandomForestClassifier()s\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'best_AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b22128",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_lr_mi = IterativeImputer(estimator=LinearRegression())\n",
    "\n",
    "imputed_data_lr_mi = imputer_lr_mi.fit_transform(feature3[continuous_features3])\n",
    "\n",
    "imputed_df_lr_mi = pd.DataFrame(imputed_data_lr_mi, columns=feature3[continuous_features3].columns)\n",
    "imputed_df_lr_mi = pd.concat([imputed_df_lr_mi,feature3[categorical_features3]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imputed_df_lr_mi, y['是否在28天内死亡'], test_size=0.3)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f'best_AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1844da",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_knn_mi = IterativeImputer(estimator=KNeighborsRegressor())\n",
    "\n",
    "imputed_data_knn_mi = imputer_knn_mi.fit_transform(feature3[continuous_features3].astype(float))\n",
    "\n",
    "imputed_df_knn_mi = pd.DataFrame(imputed_data_knn_mi, columns=feature3[continuous_features3].columns)\n",
    "imputed_df_knn_mi = pd.concat([imputed_df_knn_mi,feature3[categorical_features3]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imputed_df_knn_mi, y['是否在28天内死亡'], test_size=0.3)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'best_AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae87cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_rr = SimpleImputer(strategy='mean')\n",
    "df_imputed_rr = pd.DataFrame(imputer_rr.fit_transform(feature3[continuous_features3]),columns=feature3[continuous_features3].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_col in feature3[continuous_features3].columns:\n",
    "    X = df_imputed_rr.drop(target_col, axis=1).values  \n",
    "    y_1 = df_imputed_rr[target_col]  \n",
    "\n",
    "    ridge_model = Ridge()  \n",
    "    ridge_model.fit(X, y_1)\n",
    "    \n",
    "    y_pred = ridge_model.predict(X)\n",
    "    \n",
    "    df_imputed_rr[target_col] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df_rr = pd.concat([df_imputed_rr,feature3[categorical_features3]],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imputed_df_rr, y['是否在28天内死亡'], test_size=0.3)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'best_AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87b2e3-2292-4c4a-8704-dab13e9ffdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = feature4.corr().abs()\n",
    "\n",
    "high_corr_vars = np.where(corr_matrix >= 0.7)\n",
    "high_corr_vars = [(corr_matrix.index[x], corr_matrix.columns[y]) for x, y in zip(*high_corr_vars) if x != y and x < y]\n",
    "print(high_corr_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(series):\n",
    "    value_counts = series.value_counts(normalize=True, dropna=False)\n",
    "    entropy = -(value_counts * np.log2(value_counts + 1e-10)).sum()\n",
    "    return entropy\n",
    "\n",
    "feature_entropies = model_data.drop(['是否在28天内死亡','就诊标识（医渡云计算）'],axis=1).apply(calculate_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "97adb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "entr_result = pd.DataFrame(feature_entropies).reset_index()\n",
    "entr_result.columns=['变量','熵值']\n",
    "entr_result = entr_result.sort_values(by='熵值',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf534f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "\n",
    "for i in np.arange(2,52,2):  \n",
    "    top_variables = entr_result['变量'].head(i).tolist()\n",
    "    X = model_data[top_variables]\n",
    "    y = model_data['是否在28天内死亡']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    auc_scores.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_result = pd.DataFrame({'变量个数':np.arange(2,52,2),'AUC':auc_scores,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da03a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "ax.plot(auc_result['变量个数'], auc_result['AUC'], marker='o', linestyle='-', color='b')\n",
    "\n",
    "ax.set_xlabel('variables_number')\n",
    "ax.set_ylabel('AUC_score')\n",
    "\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.axhline(y=0.6, color='gray', linestyle='--', linewidth=0.5) \n",
    "ax.axhline(y=0.7, color='gray', linestyle='--', linewidth=0.5)\n",
    "ax.axhline(y=0.8, color='gray', linestyle='--', linewidth=0.5)\n",
    "ax.axhline(y=0.9, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.scatter(34, auc_result[auc_result['变量个数']==34]['AUC'], color='blue', marker='*',s=200)\n",
    "plt.text(34, auc_result[auc_result['变量个数']==34]['AUC']+0.03, '(34, 0.8410)', fontsize=12, \n",
    "             ha='center', va='bottom', color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0662ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(25, 25))\n",
    "\n",
    "ax = sns.heatmap(corr_matrix_rename, cmap='Greens', annot=False, xticklabels=1, yticklabels=1) \n",
    "\n",
    "plt.rcParams['font.sans-serif'] = 'times new roman'\n",
    "plt.xticks(fontsize=24) \n",
    "plt.yticks(fontsize=24)  \n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "\n",
    "cbar.ax.tick_params(labelsize=24) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981cb554-1306-4212-9561-c2904eb74be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_important = MinMaxScaler()\n",
    "X_train_scaler = scaler_important.fit_transform(X_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd51e7-47d0-46c6-8b32-e81fb7f145e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy=1,k_neighbors=20)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaler, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f9cd1-a789-421e-93c3-4ec81b273fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier()\n",
    "param_grid_tree = {\n",
    "    'criterion':['entropy','gini'],   \n",
    "    'max_depth':np.arange(10,300,10),      \n",
    "    'max_features':['sqrt' , 'log2'], \n",
    "    'max_leaf_nodes':[10,20,30,40,50,60,70,80,90,100],  \n",
    "    'min_samples_split':[10,20,30,40,50,60,70,80,90,100],\n",
    "    'min_samples_leaf':[10,20,30,40,50,60,70,80,90,100],\n",
    "    'splitter':['best' ,'random']\n",
    "}\n",
    "\n",
    "random_search_tree = RandomizedSearchCV(model_tree, param_grid_tree,scoring='roc_auc',cv=5)\n",
    "random_search_tree.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_criterion = random_search_tree.best_params_['criterion']\n",
    "best_max_depth = random_search_tree.best_params_['max_depth']\n",
    "best_max_features = random_search_tree.best_params_['max_features']\n",
    "best_max_leaf_nodes = random_search_tree.best_params_['max_leaf_nodes']\n",
    "best_min_samples_split = random_search_tree.best_params_['min_samples_split']\n",
    "best_splitter = random_search_tree.best_params_['splitter']\n",
    "best_min_samples_leaf = random_search_tree.best_params_['min_samples_leaf']\n",
    "\n",
    "print('best_criterion：',best_criterion)\n",
    "print('best_max_depth：',best_max_depth)\n",
    "print('best_max_features：',best_max_features)\n",
    "print('best_max_leaf_nodes：',best_max_leaf_nodes)\n",
    "print('best_min_samples_split：',best_min_samples_split)\n",
    "print('best_splitter：',best_splitter)\n",
    "print('best_min_samples_leaf：',best_min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree_result = DecisionTreeClassifier(criterion=best_criterion,\n",
    "                                              max_depth=best_max_depth, max_features=best_max_features ,\n",
    "                                              max_leaf_nodes = best_max_leaf_nodes, min_samples_leaf = best_min_samples_leaf,\n",
    "                                              splitter = best_splitter,\n",
    "                                              min_samples_split = best_min_samples_split)\n",
    "model_tree_result.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_y_pred_auc = model_tree_result.predict(X_resampled)\n",
    "y_probas_tree_auc = model_tree_result.predict_proba(X_resampled)\n",
    "\n",
    "y_true = np.array(y_resampled.copy()) \n",
    "y_pred = np.array(tree_y_pred_auc.copy()) \n",
    "y_probas = np.array(y_probas_tree_auc)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "pre_scores = []\n",
    "sen_scores = []\n",
    "spe_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    sample_indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n",
    "    y_true_sample = y_true[sample_indices]\n",
    "    y_pred_sample = y_pred[sample_indices]\n",
    "    y_probas_sample = y_probas[sample_indices]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_sample, y_probas_sample[:, 1])\n",
    "    auc_scores.append(auc(fpr, tpr))\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    " \n",
    "    f1_scores.append(f1_score(y_true_sample, y_pred_sample))\n",
    "    acc_scores.append(accuracy_score(y_true_sample, y_pred_sample))\n",
    "    pre_scores.append(precision_score(y_true_sample, y_pred_sample))\n",
    "    sen_scores.append(recall_score(y_true_sample, y_pred_sample))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_sample, y_pred_sample).ravel()\n",
    "    spe_scores.append(tn / (tn + fp))\n",
    "\n",
    "auc_median = np.median(auc_scores)\n",
    "auc_lower = np.percentile(auc_scores, 2.5)\n",
    "auc_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_lower = np.percentile(f1_scores, 2.5)\n",
    "f1_upper = np.percentile(f1_scores, 97.5)\n",
    "\n",
    "acc_median = np.median(acc_scores)\n",
    "acc_lower = np.percentile(acc_scores, 2.5)\n",
    "acc_upper = np.percentile(acc_scores, 97.5)\n",
    "\n",
    "pre_median = np.median(pre_scores)\n",
    "pre_lower = np.percentile(pre_scores, 2.5)\n",
    "pre_upper = np.percentile(pre_scores, 97.5)\n",
    "\n",
    "sen_median = np.median(sen_scores)\n",
    "sen_lower = np.percentile(sen_scores, 2.5)\n",
    "sen_upper = np.percentile(sen_scores, 97.5)\n",
    "\n",
    "spe_median = np.median(spe_scores)\n",
    "spe_lower = np.percentile(spe_scores, 2.5)\n",
    "spe_upper = np.percentile(spe_scores, 97.5)\n",
    "\n",
    "print(f\"AUC: Median = {auc_median}, 95% CI = [{auc_lower}, {auc_upper}]\")\n",
    "print(f\"F1: Median = {f1_median}, 95% CI = [{f1_lower}, {f1_upper}]\")\n",
    "print(f\"ACC: Median = {acc_median}, 95% CI = [{acc_lower}, {acc_upper}]\")\n",
    "print(f\"PRE: Median = {pre_median}, 95% CI = [{pre_lower}, {pre_upper}]\")\n",
    "print(f\"SEN: Median = {sen_median}, 95% CI = [{sen_lower}, {sen_upper}]\")\n",
    "print(f\"SPE: Median = {spe_median}, 95% CI = [{spe_lower}, {spe_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebcb04-2bb8-4d67-9b55-ae92fd111677",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators':np.arange(20,100,5),  \n",
    "    'criterion':['gini','entropy'], \n",
    "    'max_depth':[10,20,30,40,50,60,70,80,90,100],  \n",
    "    'min_samples_leaf':[10,20,30,40,50,60,70,80,90,100],       \n",
    "    'min_samples_split':[10,20,30,40,50,60,70,80,90,100],   \n",
    "    'max_features':['sqrt','log2']  \n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(model_rf, param_grid_rf,scoring='roc_auc',cv=5)\n",
    "random_search_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_n_estimators = random_search_rf.best_params_['n_estimators']\n",
    "best_criterion = random_search_rf.best_params_['criterion']\n",
    "best_max_depth = random_search_rf.best_params_['max_depth']\n",
    "best_min_samples_leaf = random_search_rf.best_params_['min_samples_leaf']\n",
    "best_min_samples_split = random_search_rf.best_params_['min_samples_split']\n",
    "best_max_features = random_search_rf.best_params_['max_features']\n",
    "\n",
    "print('best_n_estimators：',best_n_estimators)\n",
    "print('best_criterion ：',best_criterion)\n",
    "print('best_max_depth：',best_max_depth)\n",
    "print('best_min_samples_leaf：',best_min_samples_leaf)\n",
    "print('best_min_samples_split：',best_min_samples_split)\n",
    "print('best_max_features：',best_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf04f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_result = RandomForestClassifier(  n_estimators=best_n_estimators, criterion=best_criterion,\n",
    "                                           max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf,\n",
    "                                           min_samples_split=best_min_samples_split, max_features=best_max_features)\n",
    "model_rf_result.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33613c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_y_pred_auc = model_rf_result.predict(X_resampled)\n",
    "y_probas_rf_auc = model_rf_result.predict_proba(X_resampled)\n",
    "\n",
    "y_true = np.array(y_resampled.copy()) \n",
    "y_pred = np.array(rf_y_pred_auc.copy()) \n",
    "y_probas = np.array(y_probas_rf_auc)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "pre_scores = []\n",
    "sen_scores = []\n",
    "spe_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    sample_indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n",
    "    y_true_sample = y_true[sample_indices]\n",
    "    y_pred_sample = y_pred[sample_indices]\n",
    "    y_probas_sample = y_probas[sample_indices]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true_sample, y_probas_sample[:, 1])\n",
    "    auc_scores.append(auc(fpr, tpr))\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    \n",
    "    f1_scores.append(f1_score(y_true_sample, y_pred_sample))\n",
    "    acc_scores.append(accuracy_score(y_true_sample, y_pred_sample))\n",
    "    pre_scores.append(precision_score(y_true_sample, y_pred_sample))\n",
    "    sen_scores.append(recall_score(y_true_sample, y_pred_sample))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_sample, y_pred_sample).ravel()\n",
    "    spe_scores.append(tn / (tn + fp))\n",
    "\n",
    "auc_median = np.median(auc_scores)\n",
    "auc_lower = np.percentile(auc_scores, 2.5)\n",
    "auc_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_lower = np.percentile(f1_scores, 2.5)\n",
    "f1_upper = np.percentile(f1_scores, 97.5)\n",
    "\n",
    "acc_median = np.median(acc_scores)\n",
    "acc_lower = np.percentile(acc_scores, 2.5)\n",
    "acc_upper = np.percentile(acc_scores, 97.5)\n",
    "\n",
    "pre_median = np.median(pre_scores)\n",
    "pre_lower = np.percentile(pre_scores, 2.5)\n",
    "pre_upper = np.percentile(pre_scores, 97.5)\n",
    "\n",
    "sen_median = np.median(sen_scores)\n",
    "sen_lower = np.percentile(sen_scores, 2.5)\n",
    "sen_upper = np.percentile(sen_scores, 97.5)\n",
    "\n",
    "spe_median = np.median(spe_scores)\n",
    "spe_lower = np.percentile(spe_scores, 2.5)\n",
    "spe_upper = np.percentile(spe_scores, 97.5)\n",
    "\n",
    "print(f\"AUC: Median = {auc_median}, 95% CI = [{auc_lower}, {auc_upper}]\")\n",
    "print(f\"F1: Median = {f1_median}, 95% CI = [{f1_lower}, {f1_upper}]\")\n",
    "print(f\"ACC: Median = {acc_median}, 95% CI = [{acc_lower}, {acc_upper}]\")\n",
    "print(f\"PRE: Median = {pre_median}, 95% CI = [{pre_lower}, {pre_upper}]\")\n",
    "print(f\"SEN: Median = {sen_median}, 95% CI = [{sen_lower}, {sen_upper}]\")\n",
    "print(f\"SPE: Median = {spe_median}, 95% CI = [{spe_lower}, {spe_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9db70-d7c4-4516-9fcb-c196f454b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBClassifier()\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'max_depth': np.arange(20,50,5),\n",
    "    'min_child_weight': np.arange(5,20,1),\n",
    "    'lambda': np.arange(10,50,5),\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'eta': np.arange(0.01,0.1,0.01),\n",
    "    'gamma': [0.001,0.005,0.01,0.05,0.1],\n",
    "    'n_estimators': np.arange(20,50,5)\n",
    "}\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(model_xgb, param_grid_xgb, scoring='roc_auc',cv=5)\n",
    "random_search_xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_n_estimators = random_search_xgb.best_params_['n_estimators']\n",
    "best_max_depth = random_search_xgb.best_params_['max_depth']\n",
    "best_min_child_weight = random_search_xgb.best_params_['min_child_weight']\n",
    "best_subsample = random_search_xgb.best_params_['subsample']\n",
    "best_colsample_bytree = random_search_xgb.best_params_['colsample_bytree']\n",
    "best_gamma = random_search_xgb.best_params_['gamma']\n",
    "best_lambda = random_search_xgb.best_params_['lambda']\n",
    "best_eta = random_search_xgb.best_params_['eta']\n",
    "\n",
    "print('best_n_estimators: ', random_search_xgb.best_params_['n_estimators'])\n",
    "print('best_max_depth: ', random_search_xgb.best_params_['max_depth'])\n",
    "print('best_min_child_weight: ', random_search_xgb.best_params_['min_child_weight'])\n",
    "print('best_subsample: ', random_search_xgb.best_params_['subsample'])\n",
    "print('best_colsample_bytree: ', random_search_xgb.best_params_['colsample_bytree'])\n",
    "print('best_gamma: ', random_search_xgb.best_params_['gamma'])\n",
    "print('best_lambda: ', random_search_xgb.best_params_['lambda'])\n",
    "print('best_eta: ', random_search_xgb.best_params_['eta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_result = XGBClassifier(n_estimators=best_n_estimators,\n",
    "                                    max_depth=best_max_depth,\n",
    "                                    min_child_weight=best_min_child_weight,\n",
    "                                    subsample=best_subsample,\n",
    "                                    colsample_bytree=best_colsample_bytree,\n",
    "                                    gamma=best_gamma,\n",
    "                                    reg_lambda=best_lambda,\n",
    "                                    eta=best_eta)\n",
    "model_xgb_result.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc933e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_y_pred_auc = model_xgb_result.predict(X_resampled)\n",
    "y_probas_xgb_auc = model_xgb_result.predict_proba(X_resampled)\n",
    "\n",
    "y_true = np.array(y_resampled.copy()) \n",
    "y_pred = np.array(xgb_y_pred_auc.copy()) \n",
    "y_probas = np.array(y_probas_xgb_auc)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "pre_scores = []\n",
    "sen_scores = []\n",
    "spe_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    sample_indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n",
    "    y_true_sample = y_true[sample_indices]\n",
    "    y_pred_sample = y_pred[sample_indices]\n",
    "    y_probas_sample = y_probas[sample_indices]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_sample, y_probas_sample[:, 1])\n",
    "    auc_scores.append(auc(fpr, tpr))\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    \n",
    "    f1_scores.append(f1_score(y_true_sample, y_pred_sample))\n",
    "    acc_scores.append(accuracy_score(y_true_sample, y_pred_sample))\n",
    "    pre_scores.append(precision_score(y_true_sample, y_pred_sample))\n",
    "    sen_scores.append(recall_score(y_true_sample, y_pred_sample))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_sample, y_pred_sample).ravel()\n",
    "    spe_scores.append(tn / (tn + fp))\n",
    "\n",
    "auc_median = np.median(auc_scores)\n",
    "auc_lower = np.percentile(auc_scores, 2.5)\n",
    "auc_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_lower = np.percentile(f1_scores, 2.5)\n",
    "f1_upper = np.percentile(f1_scores, 97.5)\n",
    "\n",
    "acc_median = np.median(acc_scores)\n",
    "acc_lower = np.percentile(acc_scores, 2.5)\n",
    "acc_upper = np.percentile(acc_scores, 97.5)\n",
    "\n",
    "pre_median = np.median(pre_scores)\n",
    "pre_lower = np.percentile(pre_scores, 2.5)\n",
    "pre_upper = np.percentile(pre_scores, 97.5)\n",
    "\n",
    "sen_median = np.median(sen_scores)\n",
    "sen_lower = np.percentile(sen_scores, 2.5)\n",
    "sen_upper = np.percentile(sen_scores, 97.5)\n",
    "\n",
    "spe_median = np.median(spe_scores)\n",
    "spe_lower = np.percentile(spe_scores, 2.5)\n",
    "spe_upper = np.percentile(spe_scores, 97.5)\n",
    "\n",
    "print(f\"AUC: Median = {auc_median}, 95% CI = [{auc_lower}, {auc_upper}]\")\n",
    "print(f\"F1: Median = {f1_median}, 95% CI = [{f1_lower}, {f1_upper}]\")\n",
    "print(f\"ACC: Median = {acc_median}, 95% CI = [{acc_lower}, {acc_upper}]\")\n",
    "print(f\"PRE: Median = {pre_median}, 95% CI = [{pre_lower}, {pre_upper}]\")\n",
    "print(f\"SEN: Median = {sen_median}, 95% CI = [{sen_lower}, {sen_upper}]\")\n",
    "print(f\"SPE: Median = {spe_median}, 95% CI = [{spe_lower}, {spe_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae30ba-e474-4f10-8173-23d03a6c2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = LGBMClassifier()\n",
    "\n",
    "param_grid_lgb = {\n",
    "\n",
    "    'boosting_type': ['gbdt','goss','dart','rf'],\n",
    "    'num_leaves': np.arange(5,50,5),\n",
    "    'max_depth': np.arange(10,50,5),\n",
    "    'learning_rate': np.arange(0.001,0.05,0.01),\n",
    "    'n_estimators': np.arange(10,50,5),\n",
    "    'subsample': np.arange(0.1,1,0.1),\n",
    "    'colsample_bytree': np.arange(0.1,1,0.1),\n",
    "    'reg_alpha': [0.01, 0.05, 0.1, 0.5 ,1],\n",
    "    'verbosity': [-1]\n",
    "}\n",
    "\n",
    "random_search_lgb = RandomizedSearchCV(model_lgb, param_grid_lgb,scoring='roc_auc',cv=5)\n",
    "random_search_lgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_boosting_type = random_search_lgb.best_params_['boosting_type']\n",
    "best_num_leaves = random_search_lgb.best_params_['num_leaves']\n",
    "best_max_depth = random_search_lgb.best_params_['max_depth']\n",
    "best_learning_rate = random_search_lgb.best_params_['learning_rate']\n",
    "best_n_estimators = random_search_lgb.best_params_['n_estimators']\n",
    "best_subsample = random_search_lgb.best_params_['subsample']\n",
    "best_colsample_bytree = random_search_lgb.best_params_['colsample_bytree']\n",
    "best_reg_alpha = random_search_lgb.best_params_['reg_alpha']\n",
    "\n",
    "print('best_boosting_type:',best_boosting_type)\n",
    "print('best_num_leaves:',best_num_leaves)\n",
    "print('best_max_depth:',best_max_depth)\n",
    "print('best_learning_rate:',best_learning_rate)\n",
    "print('best_n_estimators:',best_n_estimators)\n",
    "print('best_subsample:',best_subsample)\n",
    "print('best_colsample_bytree:',best_colsample_bytree)\n",
    "print('best_reg_alpha:',best_reg_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_result = LGBMClassifier(boosting_type=best_boosting_type,\n",
    "                                     num_leaves=best_num_leaves,\n",
    "                                     max_depth=best_max_depth,\n",
    "                                     learning_rate=best_learning_rate,\n",
    "                                     n_estimators=best_n_estimators,\n",
    "                                     subsample=best_subsample,\n",
    "                                     colsample_bytree=best_colsample_bytree,\n",
    "                                     reg_alpha=best_reg_alpha, verbosity=-1)\n",
    "\n",
    "model_lgb_result.fit(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y_pred_auc = model_lgb_result.predict(X_resampled)\n",
    "y_probas_lgb_auc = model_lgb_result.predict_proba(X_resampled)\n",
    "\n",
    "y_true = np.array(y_resampled.copy()) \n",
    "y_pred = np.array(lgb_y_pred_auc.copy()) \n",
    "y_probas = np.array(y_probas_lgb_auc)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "pre_scores = []\n",
    "sen_scores = []\n",
    "spe_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    sample_indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n",
    "    y_true_sample = y_true[sample_indices]\n",
    "    y_pred_sample = y_pred[sample_indices]\n",
    "    y_probas_sample = y_probas[sample_indices]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_sample, y_probas_sample[:, 1])\n",
    "    auc_scores.append(auc(fpr, tpr))\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    \n",
    "    f1_scores.append(f1_score(y_true_sample, y_pred_sample))\n",
    "    acc_scores.append(accuracy_score(y_true_sample, y_pred_sample))\n",
    "    pre_scores.append(precision_score(y_true_sample, y_pred_sample))\n",
    "    sen_scores.append(recall_score(y_true_sample, y_pred_sample))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_sample, y_pred_sample).ravel()\n",
    "    spe_scores.append(tn / (tn + fp))\n",
    "\n",
    "auc_median = np.median(auc_scores)\n",
    "auc_lower = np.percentile(auc_scores, 2.5)\n",
    "auc_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_lower = np.percentile(f1_scores, 2.5)\n",
    "f1_upper = np.percentile(f1_scores, 97.5)\n",
    "\n",
    "acc_median = np.median(acc_scores)\n",
    "acc_lower = np.percentile(acc_scores, 2.5)\n",
    "acc_upper = np.percentile(acc_scores, 97.5)\n",
    "\n",
    "pre_median = np.median(pre_scores)\n",
    "pre_lower = np.percentile(pre_scores, 2.5)\n",
    "pre_upper = np.percentile(pre_scores, 97.5)\n",
    "\n",
    "sen_median = np.median(sen_scores)\n",
    "sen_lower = np.percentile(sen_scores, 2.5)\n",
    "sen_upper = np.percentile(sen_scores, 97.5)\n",
    "\n",
    "spe_median = np.median(spe_scores)\n",
    "spe_lower = np.percentile(spe_scores, 2.5)\n",
    "spe_upper = np.percentile(spe_scores, 97.5)\n",
    "\n",
    "print(f\"AUC: Median = {auc_median}, 95% CI = [{auc_lower}, {auc_upper}]\")\n",
    "print(f\"F1: Median = {f1_median}, 95% CI = [{f1_lower}, {f1_upper}]\")\n",
    "print(f\"ACC: Median = {acc_median}, 95% CI = [{acc_lower}, {acc_upper}]\")\n",
    "print(f\"PRE: Median = {pre_median}, 95% CI = [{pre_lower}, {pre_upper}]\")\n",
    "print(f\"SEN: Median = {sen_median}, 95% CI = [{sen_lower}, {sen_upper}]\")\n",
    "print(f\"SPE: Median = {spe_median}, 95% CI = [{spe_lower}, {spe_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(0.01,0.5,0.01),          \n",
    "    'gamma': np.arange(0.01,0.5,0.01),        \n",
    "    'kernel': ['linear', 'rbf', 'poly'],   \n",
    "    'degree': np.arange(1,10,1),                  \n",
    "}\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(svm, param_distributions=param_grid, scoring='roc_auc', cv=5 )\n",
    "random_search_svm.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_C = random_search_svm.best_params_['C']\n",
    "best_gamma = random_search_svm.best_params_['gamma']\n",
    "best_kernel = random_search_svm.best_params_['kernel']\n",
    "best_degree = random_search_svm.best_params_['degree']\n",
    "\n",
    "\n",
    "print('best_C:',best_C)\n",
    "print('best_gamma:',best_gamma)\n",
    "print('best_kernel:',best_kernel)\n",
    "print('best_degree:',best_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99cc0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm_result = SVC (C=best_C,\n",
    "                                   gamma = best_gamma,\n",
    "                                   kernel = best_kernel,\n",
    "                                   degree = best_degree,probability=True )\n",
    "model_svm_result.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_y_pred_auc = model_svm_result.predict(X_resampled)\n",
    "y_probas_svm_auc = model_svm_result.predict_proba(X_resampled)\n",
    "\n",
    "y_true = np.array(y_resampled.copy()) \n",
    "y_pred = np.array(svm_y_pred_auc.copy()) \n",
    "y_probas = np.array(y_probas_svm_auc)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "pre_scores = []\n",
    "sen_scores = []\n",
    "spe_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    sample_indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n",
    "    y_true_sample = y_true[sample_indices]\n",
    "    y_pred_sample = y_pred[sample_indices]\n",
    "    y_probas_sample = y_probas[sample_indices]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true_sample, y_probas_sample[:, 1])\n",
    "    auc_scores.append(auc(fpr, tpr))\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    \n",
    "    f1_scores.append(f1_score(y_true_sample, y_pred_sample))\n",
    "    acc_scores.append(accuracy_score(y_true_sample, y_pred_sample))\n",
    "    pre_scores.append(precision_score(y_true_sample, y_pred_sample))\n",
    "    sen_scores.append(recall_score(y_true_sample, y_pred_sample))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_sample, y_pred_sample).ravel()\n",
    "    spe_scores.append(tn / (tn + fp))\n",
    "\n",
    "auc_median = np.median(auc_scores)\n",
    "auc_lower = np.percentile(auc_scores, 2.5)\n",
    "auc_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_lower = np.percentile(f1_scores, 2.5)\n",
    "f1_upper = np.percentile(f1_scores, 97.5)\n",
    "\n",
    "acc_median = np.median(acc_scores)\n",
    "acc_lower = np.percentile(acc_scores, 2.5)\n",
    "acc_upper = np.percentile(acc_scores, 97.5)\n",
    "\n",
    "pre_median = np.median(pre_scores)\n",
    "pre_lower = np.percentile(pre_scores, 2.5)\n",
    "pre_upper = np.percentile(pre_scores, 97.5)\n",
    "\n",
    "sen_median = np.median(sen_scores)\n",
    "sen_lower = np.percentile(sen_scores, 2.5)\n",
    "sen_upper = np.percentile(sen_scores, 97.5)\n",
    "\n",
    "spe_median = np.median(spe_scores)\n",
    "spe_lower = np.percentile(spe_scores, 2.5)\n",
    "spe_upper = np.percentile(spe_scores, 97.5)\n",
    "\n",
    "print(f\"AUC: Median = {auc_median}, 95% CI = [{auc_lower}, {auc_upper}]\")\n",
    "print(f\"F1: Median = {f1_median}, 95% CI = [{f1_lower}, {f1_upper}]\")\n",
    "print(f\"ACC: Median = {acc_median}, 95% CI = [{acc_lower}, {acc_upper}]\")\n",
    "print(f\"PRE: Median = {pre_median}, 95% CI = [{pre_lower}, {pre_upper}]\")\n",
    "print(f\"SEN: Median = {sen_median}, 95% CI = [{sen_lower}, {sen_upper}]\")\n",
    "print(f\"SPE: Median = {spe_median}, 95% CI = [{spe_lower}, {spe_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03502b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "\n",
    "param_dist = {\n",
    "\n",
    "    'alpha': np.linspace(0.1, 1, 10), \n",
    "    'binarize': [None] + list(np.linspace(0.0, 1.0, 11)) \n",
    "\n",
    "}\n",
    "\n",
    "random_search_nb = RandomizedSearchCV(nb, param_distributions=param_dist, scoring='roc_auc', cv=5)\n",
    "random_search_nb.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_alpha = random_search_nb.best_params_['alpha']\n",
    "best_binarize = random_search_nb.best_params_['binarize']\n",
    "\n",
    "\n",
    "print('best_alpha:',best_alpha)\n",
    "print('best_binarize:',best_binarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe33cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb_result = BernoulliNB(alpha=best_alpha,binarize = best_binarize)\n",
    "model_nb_result.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_y_pred_auc = model_nb_result.predict(X_resampled)\n",
    "y_probas_nb_auc = model_nb_result.predict_proba(X_resampled)\n",
    "\n",
    "y_true = np.array(y_resampled.copy()) \n",
    "y_pred = np.array(nb_y_pred_auc.copy()) \n",
    "y_probas = np.array(y_probas_nb_auc)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "pre_scores = []\n",
    "sen_scores = []\n",
    "spe_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    sample_indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n",
    "    y_true_sample = y_true[sample_indices]\n",
    "    y_pred_sample = y_pred[sample_indices]\n",
    "    y_probas_sample = y_probas[sample_indices]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true_sample, y_probas_sample[:, 1])\n",
    "    auc_scores.append(auc(fpr, tpr))\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    f1_scores.append(f1_score(y_true_sample, y_pred_sample))\n",
    "    acc_scores.append(accuracy_score(y_true_sample, y_pred_sample))\n",
    "    pre_scores.append(precision_score(y_true_sample, y_pred_sample))\n",
    "    sen_scores.append(recall_score(y_true_sample, y_pred_sample))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_sample, y_pred_sample).ravel()\n",
    "    spe_scores.append(tn / (tn + fp))\n",
    "\n",
    "auc_median = np.median(auc_scores)\n",
    "auc_lower = np.percentile(auc_scores, 2.5)\n",
    "auc_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_lower = np.percentile(f1_scores, 2.5)\n",
    "f1_upper = np.percentile(f1_scores, 97.5)\n",
    "\n",
    "acc_median = np.median(acc_scores)\n",
    "acc_lower = np.percentile(acc_scores, 2.5)\n",
    "acc_upper = np.percentile(acc_scores, 97.5)\n",
    "\n",
    "pre_median = np.median(pre_scores)\n",
    "pre_lower = np.percentile(pre_scores, 2.5)\n",
    "pre_upper = np.percentile(pre_scores, 97.5)\n",
    "\n",
    "sen_median = np.median(sen_scores)\n",
    "sen_lower = np.percentile(sen_scores, 2.5)\n",
    "sen_upper = np.percentile(sen_scores, 97.5)\n",
    "\n",
    "spe_median = np.median(spe_scores)\n",
    "spe_lower = np.percentile(spe_scores, 2.5)\n",
    "spe_upper = np.percentile(spe_scores, 97.5)\n",
    "\n",
    "print(f\"AUC: Median = {auc_median}, 95% CI = [{auc_lower}, {auc_upper}]\")\n",
    "print(f\"F1: Median = {f1_median}, 95% CI = [{f1_lower}, {f1_upper}]\")\n",
    "print(f\"ACC: Median = {acc_median}, 95% CI = [{acc_lower}, {acc_upper}]\")\n",
    "print(f\"PRE: Median = {pre_median}, 95% CI = [{pre_lower}, {pre_upper}]\")\n",
    "print(f\"SEN: Median = {sen_median}, 95% CI = [{sen_lower}, {sen_upper}]\")\n",
    "print(f\"SPE: Median = {spe_median}, 95% CI = [{spe_lower}, {spe_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(10,100,10),\n",
    "    'learning_rate': np.arange(0.01,0.1,10),\n",
    "    'max_depth': np.arange(2,20,1),\n",
    "    'subsample':[0.4,0.5,0.6,0.7,0.8],\n",
    "    'min_samples_split':np.arange(2,20,2),\n",
    "    'min_samples_leaf':np.arange(20,50,5),\n",
    "    'max_features':['sqrt','log2']\n",
    "    }\n",
    "\n",
    "\n",
    "random_search_gbdt = RandomizedSearchCV(estimator = gb_clf, param_distributions=param_grid, scoring='roc_auc', cv=5)\n",
    "random_search_gbdt.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_n_estimators = random_search_gbdt.best_params_['n_estimators']\n",
    "best_learning_rate = random_search_gbdt.best_params_['learning_rate']\n",
    "best_max_depth = random_search_gbdt.best_params_['max_depth']\n",
    "best_subsample = random_search_gbdt.best_params_['subsample']\n",
    "best_min_samples_split = random_search_gbdt.best_params_['min_samples_split']\n",
    "best_min_samples_leaf = random_search_gbdt.best_params_['min_samples_leaf']\n",
    "best_max_features = random_search_gbdt.best_params_['max_features']\n",
    "\n",
    "print('best_n_estimators:',best_n_estimators)\n",
    "print('best_learning_rate:',best_learning_rate)\n",
    "print('best_max_depth:',best_max_depth)\n",
    "print('best_subsample:',best_subsample)\n",
    "print('best_min_samples_split:',best_min_samples_split)\n",
    "print('best_min_samples_leaf:',best_min_samples_leaf)\n",
    "print('best_max_features:',best_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbdt_result = GradientBoostingClassifier(n_estimators = best_n_estimators,\n",
    "                                                learning_rate = best_learning_rate,\n",
    "                                                max_depth = best_max_depth,\n",
    "                                                subsample = best_subsample,\n",
    "                                                min_samples_split = best_min_samples_split,\n",
    "                                                min_samples_leaf = best_min_samples_leaf,\n",
    "                                                max_features = best_max_features)\n",
    "model_gbdt_result.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_y_pred_auc = model_gbdt_result.predict(X_resampled)\n",
    "y_probas_gbdt_auc = model_gbdt_result.predict_proba(X_resampled)\n",
    "\n",
    "y_true = np.array(y_resampled.copy()) \n",
    "y_pred = np.array(gbdt_y_pred_auc.copy()) \n",
    "y_probas = np.array(y_probas_gbdt_auc)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "pre_scores = []\n",
    "sen_scores = []\n",
    "spe_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    sample_indices = np.random.choice(len(y_true), size=len(y_true), replace=True)\n",
    "    y_true_sample = y_true[sample_indices]\n",
    "    y_pred_sample = y_pred[sample_indices]\n",
    "    y_probas_sample = y_probas[sample_indices]\n",
    "    \n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true_sample, y_probas_sample[:, 1])\n",
    "    auc_scores.append(auc(fpr, tpr))\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    \n",
    "\n",
    "    f1_scores.append(f1_score(y_true_sample, y_pred_sample))\n",
    "    acc_scores.append(accuracy_score(y_true_sample, y_pred_sample))\n",
    "    pre_scores.append(precision_score(y_true_sample, y_pred_sample))\n",
    "    sen_scores.append(recall_score(y_true_sample, y_pred_sample))\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_sample, y_pred_sample).ravel()\n",
    "    spe_scores.append(tn / (tn + fp))\n",
    "\n",
    "auc_median = np.median(auc_scores)\n",
    "auc_lower = np.percentile(auc_scores, 2.5)\n",
    "auc_upper = np.percentile(auc_scores, 97.5)\n",
    "\n",
    "f1_median = np.median(f1_scores)\n",
    "f1_lower = np.percentile(f1_scores, 2.5)\n",
    "f1_upper = np.percentile(f1_scores, 97.5)\n",
    "\n",
    "acc_median = np.median(acc_scores)\n",
    "acc_lower = np.percentile(acc_scores, 2.5)\n",
    "acc_upper = np.percentile(acc_scores, 97.5)\n",
    "\n",
    "pre_median = np.median(pre_scores)\n",
    "pre_lower = np.percentile(pre_scores, 2.5)\n",
    "pre_upper = np.percentile(pre_scores, 97.5)\n",
    "\n",
    "sen_median = np.median(sen_scores)\n",
    "sen_lower = np.percentile(sen_scores, 2.5)\n",
    "sen_upper = np.percentile(sen_scores, 97.5)\n",
    "\n",
    "spe_median = np.median(spe_scores)\n",
    "spe_lower = np.percentile(spe_scores, 2.5)\n",
    "spe_upper = np.percentile(spe_scores, 97.5)\n",
    "\n",
    "print(f\"AUC: Median = {auc_median}, 95% CI = [{auc_lower}, {auc_upper}]\")\n",
    "print(f\"F1: Median = {f1_median}, 95% CI = [{f1_lower}, {f1_upper}]\")\n",
    "print(f\"ACC: Median = {acc_median}, 95% CI = [{acc_lower}, {acc_upper}]\")\n",
    "print(f\"PRE: Median = {pre_median}, 95% CI = [{pre_lower}, {pre_upper}]\")\n",
    "print(f\"SEN: Median = {sen_median}, 95% CI = [{sen_lower}, {sen_upper}]\")\n",
    "print(f\"SPE: Median = {spe_median}, 95% CI = [{spe_lower}, {spe_upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43169c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Tree_auc':[0.6978,0.4262,0.6744,0.3359,0.5846,0.6976],\n",
    "                'RF_auc':[0.6973,0.4472,0.6671,0.3435,0.6458,0.6745],\n",
    "                'XGB_auc':[0.7175,0.4561,0.6541,0.3388,0.7010,0.6416],\n",
    "                'LGB_auc':[0.7103,0.4545,0.6324,0.3295,0.7355,0.6048],\n",
    "                'SVM_auc':[0.6979,0.2957,0.7670,0.4020,0.2357,0.9070],\n",
    "                'NB_AUC':[0.7052,0.4561,0.6454,0.3357,0.7143,0.6275],\n",
    "                'GBDT_AUC':[0.7253,0.4529,0.6874,0.3550,0.6233,0.7032]\n",
    "                },index=['AUC','F1','ACC','PRE','SEN','SPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eda356",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = results_1 / np.sqrt((results_1 ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08510d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topsis(data, weight=None):\n",
    " \n",
    "    Z = pd.DataFrame([data.max(), data.min()], index=['正理想解', '负理想解'])\n",
    " \n",
    "    weight = entropyWeight(data) if weight is None else np.array(weight)\n",
    "    Result = data.copy()\n",
    "    Result['正理想解'] = np.sqrt(((data - Z.loc['正理想解']) ** 2 * weight).sum(axis=1))\n",
    "    Result['负理想解'] = np.sqrt(((data - Z.loc['负理想解']) ** 2 * weight).sum(axis=1))\n",
    " \n",
    "    Result['综合得分指数'] = Result['负理想解'] / (Result['负理想解'] + Result['正理想解'])  \n",
    "    Result['排序'] = Result.rank(ascending=False)['综合得分指数']\n",
    " \n",
    "    return Result, Z, weight\n",
    " \n",
    "weight = [1/6,1/6,1/6,1/6,1/6,1/6,]\n",
    "Result, Z, weight = topsis(results_1, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2b799e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_model(data):\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    y_pred_tree = model_tree_result.predict(data)\n",
    "    y_proba_tree = model_tree_result.predict_proba(data)[:,1]\n",
    "\n",
    "    y_pred_rf = model_rf_result.predict(data)\n",
    "    y_proba_rf = model_rf_result.predict_proba(data)[:,1]   \n",
    "\n",
    "    y_pred_xgb = model_xgb_result.predict(data)\n",
    "    y_proba_xgb = model_xgb_result.predict_proba(data)[:,1] \n",
    "\n",
    "    y_pred_lgb = model_lgb_result.predict(data)\n",
    "    y_proba_lgb = model_lgb_result.predict_proba(data)[:,1] \n",
    "\n",
    "    y_pred_svm = model_svm_result.predict(data)\n",
    "    y_proba_svm = model_svm_result.predict_proba(data)[:,1] \n",
    "\n",
    "    y_pred_nb = model_nb_result.predict(data)\n",
    "    y_proba_nb = model_nb_result.predict_proba(data)[:,1]      \n",
    "\n",
    "    y_pred_gbdt = model_gbdt_result.predict(data)\n",
    "    y_proba_gbdt = model_gbdt_result.predict_proba(data)[:,1]\n",
    "   \n",
    "    result['y_probas'] = y_proba_tree*0.141+y_proba_rf*0.153+y_proba_xgb*0.155+y_proba_lgb*0.151+y_proba_svm*0.089+y_proba_nb*0.153+y_proba_gbdt*0.158\n",
    " \n",
    "    result.loc[result['y_probas'] <= 0.5,'y_pred'] = np.int(0)\n",
    "    result.loc[result['y_probas']  > 0.5,'y_pred'] = np.int(1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80aae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
